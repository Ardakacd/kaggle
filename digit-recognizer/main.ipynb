{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyQC-VJrsngt",
        "outputId": "07300328-1f20-42b4-b6ed-e25695f38598"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Loss: 0.3718, Accuracy: 87.73%\n",
            "Epoch [2/10], Loss: 0.1122, Accuracy: 96.77%\n",
            "Epoch [3/10], Loss: 0.0948, Accuracy: 97.33%\n",
            "Epoch [4/10], Loss: 0.0856, Accuracy: 97.61%\n",
            "Epoch [5/10], Loss: 0.0829, Accuracy: 97.73%\n",
            "Epoch [6/10], Loss: 0.0727, Accuracy: 98.06%\n",
            "Epoch [7/10], Loss: 0.0748, Accuracy: 98.01%\n",
            "Epoch [8/10], Loss: 0.0596, Accuracy: 98.41%\n",
            "Epoch [9/10], Loss: 0.0656, Accuracy: 98.21%\n",
            "Epoch [10/10], Loss: 0.0583, Accuracy: 98.38%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.models import alexnet\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class MNISTCSV(Dataset):\n",
        "    def __init__(self, csv_file, is_train, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.is_train = is_train\n",
        "        if is_train:\n",
        "          self.labels = self.data['label'].values\n",
        "          self.images = self.data.drop('label', axis=1).values.reshape(-1, 28, 28).astype(np.uint8)\n",
        "        else:\n",
        "          self.images = self.data.values.reshape(-1, 28, 28).astype(np.uint8)\n",
        "        self.is_train = is_train\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "          return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.fromarray(self.images[idx])\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        if self.is_train:\n",
        "          label = self.labels[idx]\n",
        "          return img, label\n",
        "        else:\n",
        "          return img\n",
        "\n",
        "# Step 1: Define transforms for AlexNet\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),                  # Resize to 224x224 for AlexNet\n",
        "    transforms.Grayscale(num_output_channels=3),    # Convert 1-channel to 3-channel\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], # ImageNet normalization\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_dir = 'train.csv'\n",
        "test_dir = 'test.csv'\n",
        "\n",
        "train_dataset = MNISTCSV(train_dir, True, transform=transform)\n",
        "test_dataset = MNISTCSV(test_dir, False, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "\n",
        "model = alexnet(pretrained=False)\n",
        "model.classifier[6] = nn.Linear(4096, 10)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {100. * correct / total:.2f}%\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mvsEe0hurBFg"
      },
      "outputs": [],
      "source": [
        "predictions = []\n",
        "\n",
        "for images in test_loader:\n",
        "  outputs = model(images.to(device))\n",
        "  _, predicted = outputs.max(1)\n",
        "  predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "\n",
        "submission_df = pd.DataFrame({'ImageId': range(1, len(predictions) + 1), 'Label': predictions})\n",
        "submission_df.to_csv('submission.csv', index=False)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
